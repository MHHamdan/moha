<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.2.2">Jekyll</generator><link href="http://localhost:4001/moha/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4001/moha/" rel="alternate" type="text/html" /><updated>2024-06-28T01:14:12-04:00</updated><id>http://localhost:4001/moha/feed.xml</id><title type="html">Mohammed Hamdan’s AI Blog</title><subtitle>A blog about AI, Machine Learning, and Computer Vision.
</subtitle><entry><title type="html">Mastering Derivatives for Backpropagation in Neural Networks</title><link href="http://localhost:4001/moha/calculus/optimization/2024/06/28/derivatives-backpropagation.html" rel="alternate" type="text/html" title="Mastering Derivatives for Backpropagation in Neural Networks" /><published>2024-06-28T08:00:00-04:00</published><updated>2024-06-28T08:00:00-04:00</updated><id>http://localhost:4001/moha/calculus/optimization/2024/06/28/derivatives-backpropagation</id><content type="html" xml:base="http://localhost:4001/moha/calculus/optimization/2024/06/28/derivatives-backpropagation.html"><![CDATA[<p>Understanding derivatives is crucial in the context of backpropagation in neural networks. These derivatives form the mathematical foundation for computing gradients, which are essential for training neural networks.</p>

<h2 id="backpropagation-in-neural-networks">Backpropagation in Neural Networks</h2>

<p>Backpropagation is the process of training a neural network by adjusting weights based on the error rate obtained in the previous epoch (iteration). The main goal is to minimize the error by using the gradient descent algorithm. Here’s how the derivatives relate to backpropagation:</p>

<h3 id="chain-rule">Chain Rule</h3>

<p>The chain rule of calculus is pivotal in backpropagation. It allows us to compute the derivative of a composite function. For example, if we have ( z = f(g(x)) ), the chain rule states that:
[
\frac{dz}{dx} = \frac{dz}{dg} \cdot \frac{dg}{dx}
]</p>

<h3 id="activation-functions">Activation Functions</h3>

<p>The derivatives listed (e.g., exponential, logarithmic, trigonometric) are often used as activation functions or in the loss function calculations. Understanding their derivatives is crucial for calculating the gradients:
[
\frac{d}{dx} e^x = e^x
]
[
\frac{d}{dx} \log x = \frac{1}{x}
]
[
\frac{d}{dx} \sin x = \cos x
]</p>

<h3 id="gradient-descent">Gradient Descent</h3>

<p>During backpropagation, we need to compute the gradient of the loss function with respect to each weight in the network. These gradients are used to update the weights to minimize the loss. For instance:</p>
<ul>
  <li>If the loss function involves an exponential term ( e^{f(x)} ), its derivative ( e^{f(x)} f’(x) ) will be part of the gradient computation.</li>
</ul>

<h3 id="complex-functions">Complex Functions</h3>

<p>For complex functions, such as compositions involving trigonometric and inverse trigonometric functions, their derivatives are essential. For example:
[
\frac{d}{dx} \tan x = \sec^2 x
]
[
\frac{d}{dx} \sin^{-1} x = \frac{1}{\sqrt{1 - x^2}}
]</p>

<h2 id="application-in-backpropagation">Application in Backpropagation</h2>

<ul>
  <li><strong>Forward Pass</strong>: Compute the output of each neuron in the network.</li>
  <li><strong>Loss Computation</strong>: Calculate the loss using a suitable loss function (e.g., Mean Squared Error).</li>
  <li><strong>Backward Pass</strong>:
    <ul>
      <li>Compute the gradient of the loss function with respect to the output.</li>
      <li>Use the chain rule to propagate this gradient backward through the network, layer by layer, to compute the gradient with respect to each weight.</li>
      <li>Apply the derivative rules (as shown in the image) to determine these gradients accurately.</li>
    </ul>
  </li>
</ul>

<p>In summary, these derivatives form the building blocks for calculating gradients during backpropagation. Accurate gradient computation is vital for effectively training neural networks and optimizing their performance.</p>

<h2 id="exponential-and-logarithmic-functions">Exponential and Logarithmic Functions</h2>

<ol>
  <li>
    <p><strong>Exponential Functions</strong>:
[
\frac{d}{dx} e^x = e^x
]
[
\frac{d}{dx} e^{ax} = ae^{ax}
]
[
\frac{d}{dx} a^x = a^x \ln a
]</p>
  </li>
  <li>
    <p><strong>Logarithmic Functions</strong>:
[
\frac{d}{dx} \ln x = \frac{1}{x}
]
[
\frac{d}{dx} \log_a x = \frac{1}{x \ln a}
]</p>
  </li>
</ol>

<h2 id="power-functions">Power Functions</h2>

<p>[
\frac{d}{dx} x^n = nx^{n-1}
]</p>

<h2 id="trigonometric-functions">Trigonometric Functions</h2>

<ol>
  <li>
    <p><strong>Basic Trigonometric Functions</strong>:
[
\frac{d}{dx} \sin x = \cos x
]
[
\frac{d}{dx} \cos x = -\sin x
]
[
\frac{d}{dx} \tan x = \sec^2 x
]</p>
  </li>
  <li>
    <p><strong>Reciprocal Trigonometric Functions</strong>:
[
\frac{d}{dx} \csc x = -\csc x \cot x
]
[
\frac{d}{dx} \sec x = \sec x \tan x
]
[
\frac{d}{dx} \cot x = -\csc^2 x
]</p>
  </li>
</ol>

<h2 id="inverse-trigonometric-functions">Inverse Trigonometric Functions</h2>

<p>[
\frac{d}{dx} \sin^{-1} x = \frac{1}{\sqrt{1-x^2}}
]
[
\frac{d}{dx} \cos^{-1} x = -\frac{1}{\sqrt{1-x^2}}
]
[
\frac{d}{dx} \tan^{-1} x = \frac{1}{1+x^2}
]</p>

<p>Thank you for visiting my blog. Feel free to reach out through my social media links provided in the navigation bar.</p>]]></content><author><name></name></author><category term="Calculus/Optimization" /><summary type="html"><![CDATA[Understanding derivatives is crucial in the context of backpropagation in neural networks. These derivatives form the mathematical foundation for computing gradients, which are essential for training neural networks.]]></summary></entry><entry><title type="html">My First Blog Post</title><link href="http://localhost:4001/moha/calculus/optimization/2024/06/28/first-post.html" rel="alternate" type="text/html" title="My First Blog Post" /><published>2024-06-28T08:00:00-04:00</published><updated>2024-06-28T08:00:00-04:00</updated><id>http://localhost:4001/moha/calculus/optimization/2024/06/28/first-post</id><content type="html" xml:base="http://localhost:4001/moha/calculus/optimization/2024/06/28/first-post.html"><![CDATA[<p>Welcome to my first blog post! This is where I will share my journey in AI, Machine Learning, and Computer Vision. Stay tuned for more updates and insights!</p>

<h2 id="introduction">Introduction</h2>

<p>In this blog, I will discuss various topics related to AI, Machine Learning, and Computer Vision. I aim to provide valuable content for both beginners and experts in the field.</p>

<h2 id="topics-to-cover">Topics to Cover</h2>

<ul>
  <li>AI fundamentals</li>
  <li>Machine Learning algorithms</li>
  <li>Computer Vision techniques</li>
  <li>Latest research and advancements</li>
</ul>

<p>Thank you for visiting my blog. Feel free to reach out through my social media links provided in the navigation bar.</p>]]></content><author><name></name></author><category term="Calculus/Optimization" /><summary type="html"><![CDATA[Welcome to my first blog post! This is where I will share my journey in AI, Machine Learning, and Computer Vision. Stay tuned for more updates and insights!]]></summary></entry><entry><title type="html">Mastering Derivatives for Backpropagation in Neural Networks</title><link href="http://localhost:4001/moha/calculus/optimization/2024/06/28/math-post.html" rel="alternate" type="text/html" title="Mastering Derivatives for Backpropagation in Neural Networks" /><published>2024-06-28T08:00:00-04:00</published><updated>2024-06-28T08:00:00-04:00</updated><id>http://localhost:4001/moha/calculus/optimization/2024/06/28/math-post</id><content type="html" xml:base="http://localhost:4001/moha/calculus/optimization/2024/06/28/math-post.html"><![CDATA[<p>This post outlines fundamental derivatives of various functions, which are essential in calculus. In the context of backpropagation in neural networks, these derivatives represent the mathematical foundation for computing gradients.</p>

<h3 id="backpropagation-in-neural-networks">Backpropagation in Neural Networks</h3>

<p>Backpropagation is the process of training a neural network by adjusting weights based on the error rate obtained in the previous epoch (iteration). The main goal is to minimize the error by using the gradient descent algorithm. Here’s how the derivatives shown in the image relate to backpropagation:</p>

<ol>
  <li>
    <p><strong>Chain Rule</strong>: The chain rule of calculus is pivotal in backpropagation. It allows us to compute the derivative of a composite function. For example, if we have ( z = f(g(x)) ), the chain rule states that:
[
\frac{dz}{dx} = \frac{dz}{dg} \cdot \frac{dg}{dx}
]</p>
  </li>
  <li><strong>Activation Functions</strong>: The derivatives listed (e.g., exponential, logarithmic, trigonometric) are often used as activation functions or in the loss function calculations. Understanding their derivatives is crucial for calculating the gradients:
    <ul>
      <li>( \frac{d}{dx} e^x = e^x )</li>
      <li>( \frac{d}{dx} \log x = \frac{1}{x} )</li>
      <li>( \frac{d}{dx} \sin x = \cos x )</li>
    </ul>
  </li>
  <li><strong>Gradient Descent</strong>: During backpropagation, we need to compute the gradient of the loss function with respect to each weight in the network. These gradients are used to update the weights to minimize the loss. For instance:
    <ul>
      <li>If the loss function involves an exponential term ( e^{f(x)} ), its derivative ( e^{f(x)} f’(x) ) will be part of the gradient computation.</li>
    </ul>
  </li>
  <li><strong>Complex Functions</strong>: For complex functions, such as compositions involving trigonometric and inverse trigonometric functions, their derivatives are essential. For example:
    <ul>
      <li>( \frac{d}{dx} \tan x = \sec^2 x )</li>
      <li>( \frac{d}{dx} \sin^{-1} x = \frac{1}{\sqrt{1 - x^2}} )</li>
    </ul>
  </li>
</ol>

<h3 id="application-in-backpropagation">Application in Backpropagation</h3>

<ul>
  <li><strong>Forward Pass</strong>: Compute the output of each neuron in the network.</li>
  <li><strong>Loss Computation</strong>: Calculate the loss using a suitable loss function (e.g., Mean Squared Error).</li>
  <li><strong>Backward Pass</strong>:
    <ul>
      <li>Compute the gradient of the loss function with respect to the output.</li>
      <li>Use the chain rule to propagate this gradient backward through the network, layer by layer, to compute the gradient with respect to each weight.</li>
      <li>Apply the derivative rules (as shown in the image) to determine these gradients accurately.</li>
    </ul>
  </li>
</ul>

<p>In summary, these derivatives form the building blocks for calculating gradients during backpropagation. Accurate gradient computation is vital for effectively training neural networks and optimizing their performance.</p>

<p>To see practical implementations and visualizations of these derivative rules, please check our <a href="https://github.com/MHHamdan/your-notebook-link">Jupyter Notebook</a>.</p>]]></content><author><name></name></author><category term="calculus" /><category term="optimization" /><summary type="html"><![CDATA[This post outlines fundamental derivatives of various functions, which are essential in calculus. In the context of backpropagation in neural networks, these derivatives represent the mathematical foundation for computing gradients.]]></summary></entry><entry><title type="html">Test Post</title><link href="http://localhost:4001/moha/testing/2024/06/28/test-post.html" rel="alternate" type="text/html" title="Test Post" /><published>2024-06-28T08:00:00-04:00</published><updated>2024-06-28T08:00:00-04:00</updated><id>http://localhost:4001/moha/testing/2024/06/28/test-post</id><content type="html" xml:base="http://localhost:4001/moha/testing/2024/06/28/test-post.html"><![CDATA[<p>This is a test post to debug the issue with blog posts not appearing on the GitHub Pages site.</p>

<h2 id="heading">Heading</h2>

<p>This is a sample paragraph to ensure that the content is being rendered correctly.</p>]]></content><author><name></name></author><category term="testing" /><summary type="html"><![CDATA[This is a test post to debug the issue with blog posts not appearing on the GitHub Pages site.]]></summary></entry></feed>